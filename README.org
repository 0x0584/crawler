# **************************************************************************** #
#                                                                              #
#                                                         :::      ::::::::    #
#    README.org                                         :+:      :+:    :+:    #
#                                                     +:+ +:+         +:+      #
#    By: archid- <archid-@student.1337.ma>          +#+  +:+       +#+         #
#                                                 +#+#+#+#+#+   +#+            #
#    Created: 2020/02/10 17:53:57 by archid-           #+#    #+#              #
#    Updated: 2020/02/14 16:32:28 by archid-          ###   ########.fr        #
#                                                                              #
# **************************************************************************** #

#+TITLE: Crawler
#+AUTHOR: Anas Rchid

* About

This is a Data Engineering that consists of two things:

- Crawl a web site and extract articles, using Scarpy
- Save those articles into a hosted Mongodb database, using Pymongo
- Provide an API to get those articles from the database, using Flask

#+begin_quote
/Since the crawling is done based on CSS selection, it's obvious that not all sites have the same class. Thus, this works only for [[https://www.bbc.com/news][BBC]], but it could be easily adjusted by altering the ParseHelper class and changing the CSS properties./
#+end_quote

* The Solution

** Crawling the articles

#+begin_quote
Scrapy is a python package that provides the necessary functionality to crawl website and process the crawled data.
#+end_quote

The thing is, create a basic crawling /Item/ to hold the crawled data, then we create a custom /Spider/ to do the actual crawling. Then, we in the /Pipeline/, we insert each item into the /Mongodb/ database using /PyMongo/ driver.

- The Item is a class that defines the basic fields of an Item, title and body
- The Spider is class that defines how to =parse()= the website
- The Pipeline is class that defines how to process the Item

** The API

Flask is a micro-framework to web applications. Each time we send one of the following requests, we get a JSON response. If no results are found, it returns nothing.

- =/word/<str>= :: Fetch all the articles where their title contains the word =<str>=
- =/tag/<tag>= :: Fetch all articles that contains the tag =<tag>=
- =/text/<word>= :: Fetch all the articles where their bodies contains the word =<str>=
- =/author/<author>= :: Fetch all the articles create by =<author>= (Not all articles do, like News)

The Crawling run as a scheduled job by the server.

* How to use

Create a virtual environment and install requirements

#+BEGIN_SRC shell
  $ git clone https://github.com/0x0584/crawler.git
  $ cd crawler
  $ python3 -m env .venv
  $ source .venv/bin/activate		      # disabled with deactivate
  $ pip install -r requirements.txt
  $ cd crawler; python api.py flask start &
  $ wget 127.0.0.1:5000/tag/foobar -O foobar.json
#+END_SRC
