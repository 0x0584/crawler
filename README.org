# **************************************************************************** #
#                                                                              #
#                                                         :::      ::::::::    #
#    README.org                                         :+:      :+:    :+:    #
#                                                     +:+ +:+         +:+      #
#    By: archid- <archid-@student.1337.ma>          +#+  +:+       +#+         #
#                                                 +#+#+#+#+#+   +#+            #
#    Created: 2020/02/10 17:53:57 by archid-           #+#    #+#              #
#    Updated: 2020/02/11 22:06:50 by archid-          ###   ########.fr        #
#                                                                              #
# **************************************************************************** #

#+TITLE: Crawler
#+AUTHOR: Anas Rchid

* IN-PROGRESS Tasks

- State "IN-PROGRESS" from "TODO"       [2020-02-10 Mon 17:48]

- [ ] Write an application to crawl articles on a news website such as [theguardian.com](http://theguardian.com) or [bbc.com](http://bbc.com/) using a crawler framework such as [Scrapy](http://scrapy.org). You can use a crawl framework of your choice and build the application in Python.

- [ ] The application should cleanse the articles to obtain only information relevant to the news story, e.g. article text, author, headline, article url, etc. Use a framework such as Readability to cleanse the page of superfluous content such as advertising and HTML.

- [ ] Store the data in a hosted Mongo database, e.g. [MongoDB Atlas](https://www.mongodb.com/cloud/atlas), for subsequent search and retrieval. Ensure the URL of the article is included to enable comparison to the original.

- [ ] Write an API that provides access to the content in the mongo database. The user should be able to search for articles by keyword

* Technologies involved

- Python 3.7.6
- Scrapy 1.8.0
- PyMongo

* How to use

create a virtual environment and install requirements

#+BEGIN_SRC shell
  $ python3 -m env .venv
  $ source .venv/bin/activate		# disabled with deactivate
  $ pip install -r requirements.txt
#+END_SRC

start the crawling

#+BEGIN_SRC shell
$ scrapy crawl crawler
#+END_SRC

* Notes

This implementation works specifically on bbc.com, since it selects elements using css properties (which differ from website to another).

/I might alter it later to work a bit more generally./
